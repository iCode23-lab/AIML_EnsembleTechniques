{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB92HxSfxLPj"
   },
   "source": [
    "# Ensemble Hands On - Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy_ZAgmzxLPk"
   },
   "source": [
    "We are going to build a model that predicts if someone who seeks a loan might be a defaulter or a non-defaulter. We have several independent variables like, checking account balance, credit history, purpose, loan amount etc. For more details on the dataset, please see source at https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFyI3CAtxLPk"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NjHSaCTxLPl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use the same `credit data` used in decision tree hands-on lecture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgm9EEt5xLPo",
    "outputId": "3120b7c3-2035-4c20-9107-5db2ac686ca1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"credit.csv\"\n",
    "creditData = pd.read_csv(url)\n",
    "\n",
    "#creditData = pd.read_csv(\"credit.csv\")\n",
    "creditData.head(10) #several missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t1OBSqzxLPr",
    "outputId": "c5db5b2b-a257-4d03-b2cf-ef59e8bed4a2"
   },
   "outputs": [],
   "source": [
    "creditData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32heA0VFxLPt",
    "outputId": "f1119e0b-fa26-43ea-ba6c-f6d77ae2354c"
   },
   "outputs": [],
   "source": [
    "creditData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv_-CH_JxLPx",
    "outputId": "36a727e2-731c-4dd1-cb3f-53192e55bd1f"
   },
   "outputs": [],
   "source": [
    "creditData.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVDy-BlVxLPz"
   },
   "source": [
    "Lets convert the columns with an 'object' datatype into categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQVE5XZJxLPz",
    "outputId": "683bab0e-8751-42e5-85e0-b48d80399907"
   },
   "outputs": [],
   "source": [
    "for feature in creditData.columns: # Loop through all columns in the dataframe\n",
    "    if creditData[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        creditData[feature] = pd.Categorical(creditData[feature])# Replace strings with an integer\n",
    "creditData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf4JAhwJxLP2",
    "outputId": "4719ccee-2e82-4174-e740-ebd0d7166d2d"
   },
   "outputs": [],
   "source": [
    "print(creditData.checking_balance.value_counts())\n",
    "print(creditData.credit_history.value_counts())\n",
    "print(creditData.purpose.value_counts())\n",
    "print(creditData.savings_balance.value_counts())\n",
    "print(creditData.employment_duration.value_counts())\n",
    "print(creditData.other_credit.value_counts())\n",
    "print(creditData.housing.value_counts())\n",
    "print(creditData.job.value_counts())\n",
    "print(creditData.phone.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tgft_TIVxLP4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replaceStruct = {\n",
    "                \"checking_balance\":     {\"< 0 DM\": 1, \"1 - 200 DM\": 2 ,\"> 200 DM\": 3 ,\"unknown\":-1},\n",
    "                \"credit_history\": {\"critical\": 1, \"poor\":2 , \"good\": 3, \"very good\": 4,\"perfect\": 5},\n",
    "                 \"savings_balance\": {\"< 100 DM\": 1, \"100 - 500 DM\":2 , \"500 - 1000 DM\": 3, \"> 1000 DM\": 4,\"unknown\": -1},\n",
    "                 \"employment_duration\":     {\"unemployed\": 1, \"< 1 year\": 2 ,\"1 - 4 years\": 3 ,\"4 - 7 years\": 4 ,\"> 7 years\": 5},\n",
    "                \"phone\":     {\"no\": 1, \"yes\": 2 },\n",
    "                #\"job\":     {\"unemployed\": 1, \"unskilled\": 2, \"skilled\": 3, \"management\": 4 },\n",
    "                \"default\":     {\"no\": 0, \"yes\": 1 } \n",
    "                    }\n",
    "oneHotCols=[\"purpose\",\"housing\",\"other_credit\",\"job\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxeyJbJFxLP6",
    "outputId": "ec5bb8e3-b09d-4920-f84b-d85791f37ff4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "creditData=creditData.replace(replaceStruct)\n",
    "creditData=pd.get_dummies(creditData, columns=oneHotCols)\n",
    "creditData.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqTiaxCrxLP8",
    "outputId": "ffb94d79-da96-4923-937a-7171b38eec97"
   },
   "outputs": [],
   "source": [
    "creditData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L-oAMItxLP-"
   },
   "source": [
    "## Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When data (classification) exhibit a significant imbalance in the distribution of the target classes, it is good to use stratified sampling to ensure that relative class frequencies are approximately preserved in train and test sets. \n",
    "- This is done by setting the `stratify` parameter to target variable in the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWq265BvxLP_"
   },
   "outputs": [],
   "source": [
    "X = creditData.drop(\"default\" , axis=1)\n",
    "y = creditData.pop(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EIKHRCmxLQB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before building the model, let's create functions to calculate different metrics- Accuracy, Recall and Precision and plot the confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create confusion matrix\n",
    "def make_confusion_matrix(model,y_actual,labels=[1, 0]):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "    y_actual : ground truth  \n",
    "    \n",
    "    '''\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\n",
    "def get_metrics_score(model,flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[] \n",
    "    \n",
    "    #Predicting on train and tests\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    \n",
    "    #Accuracy of the model\n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    \n",
    "    #Recall of the model\n",
    "    train_recall = metrics.recall_score(y_train,pred_train)\n",
    "    test_recall = metrics.recall_score(y_test,pred_test)\n",
    "    \n",
    "    #Precision of the model\n",
    "    train_precision = metrics.precision_score(y_train,pred_train)\n",
    "    test_precision = metrics.precision_score(y_test,pred_test)\n",
    "    \n",
    "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n",
    "        \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n",
    "        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n",
    "        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
    "        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n",
    "        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n",
    "        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n",
    "    \n",
    "    return score_list # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "- We are going to build 2 ensemble models here - Bagging Classifier and Random Forest Classifier.\n",
    "- First, let's build these models with default parameters and then use hyperparameter tuning to optimize the model performance.\n",
    "- We will calculate all three metrics - Accuracy, Precision and Recall but the metric of interest here is recall.\n",
    "- `Recall` - It gives the ratio of True positives to Actual positives, so high Recall implies low false negatives, i.e. low chances of predicting a defaulter as non defaulter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_estimator for bagging classifier is a decision tree by default\n",
    "bagging_estimator=BaggingClassifier(random_state=1)\n",
    "bagging_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "bagging_estimator_score=get_metrics_score(bagging_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(bagging_estimator,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the random forest classifier\n",
    "rf_estimator=RandomForestClassifier(random_state=1)\n",
    "rf_estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "rf_estimator_score=get_metrics_score(rf_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(rf_estimator,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With default parameters:**\n",
    "\n",
    "- Both models - Bagging classifiers as well as random forest classifier are overfitting the train data.\n",
    "- Both models are giving similar performance in terms of accuracy but bagging classifier is giving better recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the important hyperparameters available for bagging classifier are:**\n",
    "\n",
    "- base_estimator: The base estimator to fit on random subsets of the dataset. If None(default), then the base estimator is a decision tree.\n",
    "- n_estimators: The number of trees in the forest, default = 100.\n",
    "- max_features: The number of features to consider when looking for the best split. \n",
    "- bootstrap: Whether bootstrap samples are used when building trees. If False, the entire dataset is used to build each tree, default=True.\n",
    "- bootstrap_features: If it is true, then features are drawn with replacement. Default value is False.\n",
    "- max_samples: If bootstrap is True, then the number of samples to draw from X to train each base estimator. If None (default), then draw N samples, where N is the number of observations in the train data.\n",
    "- oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy, default=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "bagging_estimator_tuned = BaggingClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "## add from article\n",
    "parameters = {'max_samples': [0.7,0.8,0.9,1], \n",
    "              'max_features': [0.7,0.8,0.9,1],\n",
    "              'n_estimators' : [10,20,30,40,50],\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "bagging_estimator_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "bagging_estimator_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check different metrics for bagging classifier with best hyperparameters and build a confusion matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "bagging_estimator_tuned_score=get_metrics_score(bagging_estimator_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(bagging_estimator_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- We can see that train accuracy and recall for the bagging classifier have increased slightly after hyperparameter tuning but the test recall has decreased.\n",
    "- The model is overfitting the data, as train accuracy and recall are much higher than the test accuracy and test recall.\n",
    "- The confusion matrix shows that the model is better at identifying non-defaulters as compared to defaulters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try using logistic regression as the base estimator for bagging classifier:\n",
    "- Now, let's try and change the `base_estimator` of the bagging classifier, which is a decision tree by default.\n",
    "- We will pass the logistic regression as the base estimator for bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_lr=BaggingClassifier(base_estimator=LogisticRegression(random_state=1),random_state=1)\n",
    "bagging_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "bagging_lr_score=get_metrics_score(bagging_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(bagging_lr,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- Bagging classifier with logistic regression as base_estimator is not overfitting the data but the test recall is very low.\n",
    "- Ensemble models are less interpretable than decision tree but bagging classifier is even less interpretable than random forest. It does not even have a feature importance attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "**Now, let's see if we can get a better model by tuning the random forest classifier. Some of the important hyperparameters available for random forest classifier are:**\n",
    "\n",
    "- n_estimators: The number of trees in the forest, default = 100.\n",
    "- max_features: The number of features to consider when looking for the best split. \n",
    "- class_weight: Weights associated with classes in the form {class_label: weight}.If not given, all classes are supposed to have weight one.  \n",
    "- For example: If the frequency of class 0 is 80% and the frequency of class 1 is 20% in the data, then class 0 will become the dominant class and the model will become biased toward the dominant classes. In this case, we can pass a dictionary {0:0.2,1:0.8} to the model to specify the weight of each class and the random forest will give more weightage to class 1. \n",
    "- bootstrap: Whether bootstrap samples are used when building trees. If False, the entire dataset is used to build each tree, default=True.\n",
    "- max_samples: If bootstrap is True, then the number of samples to draw from X to train each base estimator. If None (default), then draw N samples, where N is the number of observations in the train data.\n",
    "- oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy, default=False.\n",
    "\n",
    "- Note: A lot of hyperparameters of Decision Trees are also available to tune  Random Forest like max_depth, min_sample_split etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "rf_estimator_tuned = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "## add from article\n",
    "parameters = {\"n_estimators\": [150,200,250],\n",
    "    \"min_samples_leaf\": np.arange(5, 10),\n",
    "    \"max_features\": np.arange(0.2, 0.7, 0.1),\n",
    "    \"max_samples\": np.arange(0.3, 0.7, 0.1),\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf_estimator_tuned, parameters, scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "rf_estimator_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "rf_estimator_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "rf_estimator_tuned_score=get_metrics_score(rf_estimator_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(rf_estimator_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- We can see that random forest's performance has increased as compared to the random forest model with default parameters.\n",
    "- Model is slightly overfitting the data but not as much as the tuned bagging classifier.\n",
    "- The test recall is still very low. This means that the model is not good at identifying defaulters which is our primary aim here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try using class_weights for random forest:\n",
    "- The model performance is not very good. This may be due to the fact that the classes are imbalanced with 70% non-defaulters and 30% defaulters. \n",
    "\n",
    "- We should make the model aware that the class of interest here is 'defaulters'.\n",
    "\n",
    "- We can do so by passing the parameter `class_weights` available for random forest. This parameter is not available for the bagging classifier.\n",
    "\n",
    "- class_weight specifies the weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
    "\n",
    "- We can choose class_weights={0:0.3,1:0.7} because that is the original imbalance in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier. \n",
    "rf_estimator_weighted = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "## add from article\n",
    "parameters = {\n",
    "    \"class_weight\": [{0: 0.3, 1: 0.7}],\n",
    "    \"n_estimators\": [100,150,200,250],\n",
    "    \"min_samples_leaf\": np.arange(5, 10),\n",
    "    \"max_features\": np.arange(0.2, 0.7, 0.1),\n",
    "    \"max_samples\": np.arange(0.3, 0.7, 0.1),\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf_estimator_weighted, parameters, scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "rf_estimator_weighted = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "rf_estimator_weighted.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "rf_estimator_weighted_score=get_metrics_score(rf_estimator_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(rf_estimator_weighted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- The model accuracy has decreased a bit but the overfitting has also been reduced and the model is generalizing well.\n",
    "- The train and test recall both have increased significantly.\n",
    "- We can see from the confusion matrix that the random forest model with class weights is now better at identifying the defaulters as compared to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_estimator_weighted.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking balance, amount and months loan duration are the top 3 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all models till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining list of models\n",
    "models = [bagging_estimator,bagging_estimator_tuned,bagging_lr,rf_estimator,rf_estimator_tuned,\n",
    "          rf_estimator_weighted]\n",
    "\n",
    "# defining empty lists to add train and test results\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "recall_train = []\n",
    "recall_test = []\n",
    "precision_train = []\n",
    "precision_test = []\n",
    "\n",
    "# looping through all the models to get the accuracy, precall and precision scores\n",
    "for model in models:\n",
    "    j = get_metrics_score(model,False)\n",
    "    acc_train.append(np.round(j[0],2))\n",
    "    acc_test.append(np.round(j[1],2))\n",
    "    recall_train.append(np.round(j[2],2))\n",
    "    recall_test.append(np.round(j[3],2))\n",
    "    precision_train.append(np.round(j[4],2))\n",
    "    precision_test.append(np.round(j[5],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_frame = pd.DataFrame({'Model':['Bagging classifier with default parameters','Tuned Bagging Classifier',\n",
    "                                        'Bagging classifier with base_estimator=LR', 'Random Forest with deafult parameters',\n",
    "                                         'Tuned Random Forest Classifier','Random Forest with class_weights'], \n",
    "                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n",
    "                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n",
    "                                          'Train_Precision':precision_train,'Test_Precision':precision_test}) \n",
    "comparison_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- Hyperparameter tuning is tricky and exhaustive in the sense that there is no direct way to calculate how a change in the\n",
    "  hyperparameter value will reduce the loss of your model until you try those hyperparameters.\n",
    "- The final results depend on the parameters used/checked using GridSearchCV.\n",
    "- There may be yet better parameters which may result in a better accuracy and recall. Students can explore this further."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Decision_Tree_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
